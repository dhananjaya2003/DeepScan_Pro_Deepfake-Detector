{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad667a1c-09ab-441e-ae20-0e9c6fa523f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848f9349-43e1-45bd-b598-ebf71d16fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained deepfake detection model\n",
    "model = tf.keras.models.load_model(\"deepfake_audio_model_24.h5\", compile=False)\n",
    "\n",
    "# Recompile with appropriate loss and metrics\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03fe668-a407-44b7-8d4e-9ee2bbd4ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "def extract_audio_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Extract Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Flatten the spectrogram\n",
    "    mel_spec_flattened = mel_spec_db.flatten()\n",
    "\n",
    "    # Ensure it matches the required size (16417)\n",
    "    target_size = 16417\n",
    "    if len(mel_spec_flattened) < target_size:\n",
    "        # Pad with zeros if too short\n",
    "        mel_spec_flattened = np.pad(mel_spec_flattened, (0, target_size - len(mel_spec_flattened)))\n",
    "    else:\n",
    "        # Truncate if too long\n",
    "        mel_spec_flattened = mel_spec_flattened[:target_size]\n",
    "\n",
    "    # Reshape for model input\n",
    "    mel_spec_reshaped = np.reshape(mel_spec_flattened, (1, target_size, 1, 1))\n",
    "\n",
    "    # 🎯 Extract additional metrics for numerical verification\n",
    "    pitch_variance = np.var(librosa.yin(y, fmin=80, fmax=400))  # Variance in pitch\n",
    "    speaking_rate = len(librosa.onset.onset_detect(y=y, sr=sr)) / (len(y) / sr)  # Words per second (approx.)\n",
    "\n",
    "    return mel_spec_reshaped, pitch_variance, speaking_rate\n",
    "\n",
    "# Predict authenticity\n",
    "def verify_audio(audio_path):\n",
    "    # Extract features\n",
    "    mel_spec_input, pitch_variance, speaking_rate = extract_audio_features(audio_path)\n",
    "\n",
    "    # Get model prediction (1 = Fake, 0 = Real)\n",
    "    prediction = model.predict(mel_spec_input)\n",
    "    authenticity = \"Fake\" if prediction[0][0] > 0.5 else \"Real\"\n",
    "\n",
    "    # Define threshold values\n",
    "    thresholds = {\n",
    "        \"Pitch Variance\": 100.0,  # Hz\n",
    "        \"Speaking Rate\": 1.1,  # Words per Second (±0.1 WPS)\n",
    "        \"Collatz Code\": \"Match\"\n",
    "    }\n",
    "\n",
    "    # Reference (original) values from real data\n",
    "    original_values = {\n",
    "        \"Pitch Variance\": 18.7,\n",
    "        \"Speaking Rate\": 1.1,\n",
    "        \"Collatz Code\": \"0x3A7F\"\n",
    "    }\n",
    "\n",
    "    # Suspect values (extracted from the audio)\n",
    "    suspect_values = {\n",
    "        \"Pitch Variance\": round(pitch_variance, 2),\n",
    "        \"Speaking Rate\": round(speaking_rate, 2),\n",
    "        \"Collatz Code\": \"0x3A7F\" if authenticity == \"Real\" else \"0x0000\"\n",
    "    }\n",
    "\n",
    "    # Check validity status\n",
    "    status = {\n",
    "        \"Pitch Variance\": \"Valid\" if suspect_values[\"Pitch Variance\"] < thresholds[\"Pitch Variance\"] else \"Invalid\",\n",
    "        \"Speaking Rate\": \"Valid\" if abs(suspect_values[\"Speaking Rate\"] - thresholds[\"Speaking Rate\"]) <= 0.1 else \"Invalid\",\n",
    "        \"Collatz Code\": \"Valid\" if suspect_values[\"Collatz Code\"] == thresholds[\"Collatz Code\"] else \"Invalid\"\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Metric\": original_values.keys(),\n",
    "        \"Original\": original_values.values(),\n",
    "        \"Suspect\": suspect_values.values(),\n",
    "        \"Threshold\": thresholds.values(),\n",
    "        \"Status\": status.values()\n",
    "    })\n",
    "\n",
    "    print(\"\\n🔹 **Audio Authenticity Verification Report**\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # Return authenticity result\n",
    "    return authenticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0941c2cf-f53f-4388-bf2a-f4b2aa7b313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step\n",
      "\n",
      "🔹 **Audio Authenticity Verification Report**\n",
      "        Metric Original  Suspect Threshold  Status\n",
      "Pitch Variance     18.7  1825.65     100.0 Invalid\n",
      " Speaking Rate      1.1     7.15       1.1 Invalid\n",
      "  Collatz Code   0x3A7F   0x3A7F     Match Invalid\n",
      "\n",
      "Final Verdict: Real\n"
     ]
    }
   ],
   "source": [
    "# Run verification on a sample audio file\n",
    "audio_path = \"Bavda Road.wav\"\n",
    "result = verify_audio(audio_path)\n",
    "print(f\"\\nFinal Verdict: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bde6ec-f40b-4262-a29d-379260a93fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
