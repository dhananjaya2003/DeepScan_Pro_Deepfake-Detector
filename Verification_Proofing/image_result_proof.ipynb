{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845b13cf-b164-437d-9e86-7c3907bee3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, SwinModel\n",
    "from PIL import Image as pimg\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import Occlusion\n",
    "import tempfile\n",
    "import os\n",
    "import uuid\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import Occlusion\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    image = pimg.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    return image, inputs\n",
    "\n",
    "def get_model_features(inputs):\n",
    "    with torch.no_grad():\n",
    "        return swin(**inputs).pooler_output\n",
    "\n",
    "def get_cmap_by_label(label):\n",
    "    return \"Greens\" if label == \"Real\" else \"Reds\"\n",
    "\n",
    "def generate_occlusion_map3(img_path, label):\n",
    "    image, inputs = preprocess_image(img_path)\n",
    "    img_tensor = inputs['pixel_values'].to(device)\n",
    "    baseline = torch.zeros_like(img_tensor).to(device)\n",
    "\n",
    "    def model_wrapper(input_tensor):\n",
    "        feats = swin(pixel_values=input_tensor).pooler_output\n",
    "        return classifier(feats)\n",
    "\n",
    "    occlusion = Occlusion(model_wrapper)\n",
    "    attributions_occ = occlusion.attribute(\n",
    "        img_tensor,\n",
    "        strides=(3, 16, 16),\n",
    "        sliding_window_shapes=(3, 30, 30),\n",
    "        baselines=baseline\n",
    "    )\n",
    "\n",
    "    cmap = get_cmap_by_label(label)\n",
    "\n",
    "    # Get the fig and axis separately\n",
    "    fig, _ = viz.visualize_image_attr_multiple(\n",
    "        attributions_occ[0].cpu().permute(1, 2, 0).detach().numpy(),\n",
    "        np.array(image) / 255.0,\n",
    "        methods=[\"original_image\", \"heat_map\"],\n",
    "        signs=[\"all\", \"positive\"],\n",
    "        titles=[\"Original\", \"Occlusion Map\"],\n",
    "        cmap=cmap,\n",
    "        show_colorbar=True,\n",
    "        outlier_perc=1,\n",
    "        use_pyplot=False\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Detection Justification via Occlusion Maps\",\n",
    "        fontsize=16,\n",
    "        fontweight='bold',\n",
    "        color=\"#004830\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # Save to a temp image\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    filename = f\"occlusion_map_{uuid.uuid4().hex}.png\"\n",
    "    save_path = os.path.join(temp_dir, filename)\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f36b34-cc01-4e1a-8084-639946ee23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
